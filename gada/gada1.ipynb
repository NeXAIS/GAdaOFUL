{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import math\n",
    "import json\n",
    "import numpy as np \n",
    "import scipy\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from random import randrange\n",
    "from scipy.optimize import minimize \n",
    "\n",
    "def init_vector(dim, norm):\n",
    "    vec = 2 * np.random.rand(dim) - 1.\n",
    "    return vec * norm / math.sqrt(dim)\n",
    "\n",
    "def init_arms(dim, norm, num):\n",
    "    decision = np.random.rand(num, dim)\n",
    "    for i in range(num):\n",
    "        decision[i] = init_vector(dim, norm)\n",
    "    return decision\n",
    "\n",
    "def func(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "dim = 10\n",
    "sigma= 1\n",
    "corruption = 50\n",
    "T= 1000\n",
    "repeat = 5\n",
    "repeat_time= repeat\n",
    "actions = 20\n",
    "norm = 1\n",
    "bmu = np.ones(dim)/ math.sqrt(dim)\n",
    "print(bmu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t as t_dist\n",
    "def reward_function(chosen_arm, theta_star, flag,df=3, scale=1):\n",
    "    expected_payoff = func(chosen_arm.dot(theta_star))\n",
    "\n",
    "    noise = t_dist.rvs(df=df) * scale \n",
    "    \n",
    "    if flag==0:\n",
    "        observed_payoffs = expected_payoff + noise\n",
    "    else:\n",
    "        observed_payoffs = -expected_payoff + noise\n",
    "    return observed_payoffs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.integrate import quad  \n",
    "  \n",
    "def z_s(u, y_s, sigma_s, f):  \n",
    "    return (y_s - f(u)) / sigma_s  \n",
    "\n",
    "# Define the loss function.\n",
    "def objective_function(theta, lambda_k, B, k, t, phi_s, y_s, sigma_s, tau_s, f):  \n",
    "   \n",
    "    loss = lambda_k * k / 2 * np.linalg.norm(theta)**2  \n",
    "    \n",
    "    \n",
    "    for s in range(1, t+1 ):  \n",
    "        \n",
    "        integral_func = lambda u: (tau_s[s] * z_s(u, y_s[s], sigma_s[s], f)) / np.sqrt(tau_s[s]**2 + z_s(u, y_s[s], sigma_s[s], f)**2)  \n",
    "        integral_result, _ = quad(integral_func, 0, np.dot(phi_s[s], theta))  \n",
    "        loss += -1 / sigma_s[s] * integral_result  \n",
    "        \n",
    "    return loss \n",
    "\n",
    "\n",
    "# Define the gradient of the loss function.\n",
    "def gradient_function(theta, lambda_k, B, k, t, phi_s, y_s, sigma_s, tau_s, f):  \n",
    "    grad = lambda_k * k * theta  \n",
    "    for s in range(1, t + 1):  \n",
    "        z_s_at_upper_limit = z_s(np.dot(phi_s[s], theta), y_s[s], sigma_s[s], f)  \n",
    "        grad_term = (tau_s[s] * z_s_at_upper_limit) / np.sqrt(tau_s[s]**2 + z_s_at_upper_limit**2)  \n",
    "        grad_integral_part = -1 / sigma_s[s] * phi_s[s] * grad_term\n",
    "        grad = grad + grad_integral_part  \n",
    "    return grad  \n",
    "     \n",
    "def constraint12(theta):\n",
    "     return 1 - np.linalg.norm(theta)\n",
    "\n",
    "def project_to_unit_ball(theta):  \n",
    "\n",
    "    norm = np.linalg.norm(theta)  \n",
    "    if norm > 1:  \n",
    "        return theta / norm  \n",
    "    else:  \n",
    "        return theta  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for jjjj in range(repeat):\n",
    "    \n",
    "    number =  jjjj-repeat_time*int(jjjj /repeat_time)\n",
    "    print(corruption, number)\n",
    "    cur_crr = 1\n",
    "    decision_t = init_arms(dim, norm, actions)\n",
    "    \n",
    "    B=1\n",
    "    L=1\n",
    "    K=1/4\n",
    "    k=1/6\n",
    "    delta=0.001\n",
    "    lambda_= 0.1\n",
    "    #lambda_= dim/B**2\n",
    "    H = np.eye(dim) * lambda_\n",
    "    theta = np.zeros(dim)\n",
    "    beta = np.sqrt(lambda_)   \n",
    "    #m_0 = 1 / (6 * np.sqrt(3 * np.log(2 * T**2 / delta)))\n",
    "    m_0=1\n",
    "    m_1 = 1 / (42 * np.log(2 * T**2 / delta))\n",
    "    \n",
    "    sigma_min = 1 / np.sqrt(T)\n",
    "    print(sigma_min)\n",
    "    kappa = dim *np.log(1 + (T ) / (dim * lambda_ * sigma_min ** 2))\n",
    "    print(kappa)\n",
    "    #tau_0 = max(np.sqrt(2 * kappa), 2 * np.sqrt(dim)) / np.sqrt(np.log(2 * T**2 / delta))\n",
    "    tau_0=1\n",
    "    print(tau_0,m_0,m_1)\n",
    "    # Parameters for calculations\n",
    "    alpha = max(np.sqrt(K) / (m_1 ** 0.25 * dim ** 0.25),corruption ** 0.5 *kappa ** (-0.25))\n",
    "    print(alpha)\n",
    "    sigma_ = [None] * (T + 1)\n",
    "    w_ = [None] * (T + 1)\n",
    "    tau_ = [None] * (T + 1)\n",
    "    y_ = [None] * (T + 1) \n",
    "    var_=[None] * (T + 1)\n",
    "    phi_ = [None] * (T + 1)  \n",
    "    \n",
    "   \n",
    "    REGRET = 0\n",
    "    TOTALREGRET = []\n",
    "    for t in range(1,T+1):\n",
    "        noise=np.random.randn(actions) *sigma \n",
    "        \n",
    "        # Add corruption.\n",
    "        flag = 0\n",
    "        if cur_crr < corruption:\n",
    "            flag = 1\n",
    "\n",
    "        decision = init_arms(dim, norm, actions)\n",
    "        \n",
    "        rewardS = np.random.randn(actions)\n",
    "        reward = np.random.randn(actions)\n",
    "        optimal_reward = float(\"-inf\")\n",
    "        for arm in range(actions):\n",
    "            rewardS[arm] = noise[arm] + np.dot(decision[arm], bmu)\n",
    "            reward[arm] = rewardS[arm]\n",
    "            if np.dot(decision[arm], bmu) > optimal_reward:\n",
    "                optimal_reward = np.dot(decision[arm], bmu)\n",
    "        if flag == 1:\n",
    "            cur_crr += 1\n",
    "            for arm in range(actions):\n",
    "                reward[arm] = noise[arm] - np.dot(decision[arm], bmu)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # Find the best arm.\n",
    "        max_dot_product = float('-inf')  \n",
    "        best_i= None \n",
    "        for i in range(actions):\n",
    "            \n",
    "            # Build the confidence set.\n",
    "            def objective(theta_):  \n",
    "                return -np.dot(decision[i], theta_)  \n",
    "\n",
    "            def constraint1(theta_):  \n",
    "                return B - np.linalg.norm(theta_)  \n",
    "\n",
    "            def constraint2(theta_):  \n",
    "                return beta - np.sqrt(np.dot(np.dot(theta_ - theta, H), theta_ - theta))  \n",
    "            \n",
    "            theta0 = np.zeros_like(theta)  \n",
    "\n",
    "            con1 = {'type': 'ineq', 'fun': constraint1}  \n",
    "            con2 = {'type': 'ineq', 'fun': constraint2}  \n",
    "            cons = [con1, con2]  \n",
    "\n",
    " \n",
    "            result = minimize(objective, theta0, constraints=cons)  \n",
    "\n",
    "            optimal_theta = result.x  \n",
    "            optimal_value = -result.fun  \n",
    "            if optimal_value > max_dot_product:  \n",
    "                max_dot_product = optimal_value \n",
    "                best_i = i\n",
    "        \n",
    "        \n",
    "        phi_[t]= decision[best_i]\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        REGRET += func(optimal_reward) -func(np.dot(phi_[t], bmu))\n",
    "        \n",
    "        phi_t_H_inv_norm= np.sqrt(np.dot(np.dot(phi_[t], np.linalg.inv(H)),phi_[t]))\n",
    "        \n",
    "\n",
    "        y_[t] = reward_function(phi_[t], bmu, flag)\n",
    "        \n",
    "\n",
    "        var_[t] = sigma\n",
    "        sigma_[t] = max(var_[t], sigma_min, phi_t_H_inv_norm / m_0, alpha * (phi_t_H_inv_norm ** 0.5))\n",
    "        w_[t]= phi_t_H_inv_norm/sigma_[t]\n",
    "        tau_[t]= tau_0 * np.sqrt(1 + w_[t] ** 2) / w_[t]\n",
    "        \n",
    "        \n",
    "        theta = minimize(fun=objective_function, x0=theta, args=(lambda_, B, k, t, phi_, y_, sigma_, tau_, func),jac=gradient_function,constraints=[{'type': 'ineq', 'fun': constraint12}]).x\n",
    "        \n",
    "        #if t % 100 == 0:\n",
    "            #print(theta)\n",
    "            #print(sigma_[t],w_[t],tau_[t],phi_[t],y_[t],var_[t],phi_t_H_inv_norm,REGRET)  \n",
    "\n",
    "\n",
    "        \n",
    "        # Update matrix H.\n",
    "        H += np.outer(phi_[t], phi_[t]) / (sigma_[t] ** 2)\n",
    "       \n",
    "        #beta = (64 / k) * (2 * np.sqrt(kappa * np.log(2 * t**2 / delta)) + np.sqrt(dim * np.log(2 * t**2 / delta)) )+ 5 * np.sqrt(dim) + (4 * np.sqrt(kappa) / k)\n",
    "        beta=1.\n",
    "        if t %1 == 0:\n",
    "            print(t)\n",
    "            print(f'distance={np.linalg.norm(theta-bmu)}') \n",
    "            print(theta)\n",
    "           \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        TOTALREGRET.append(REGRET)\n",
    "    \n",
    "    \n",
    "    path  = \"GADA_lowerbound_\" + str(number) +\"_\"+ str(corruption)+\".txt\"\n",
    "    fr = open(path,'w')\n",
    "    for i in TOTALREGRET:\n",
    "        fr.write(str(i))\n",
    "        fr.write(\"\\n\")\n",
    "    fr.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jjjj in range(repeat):\n",
    "    \n",
    "    number =  jjjj-repeat_time*int(jjjj /repeat_time)\n",
    "    print(corruption, number)\n",
    "    cur_crr = 0\n",
    "    decision_t = init_arms(dim, norm, actions)\n",
    "    \n",
    "    LAMBDA = 1\n",
    "    beta=1.5\n",
    "    alpha=0.2\n",
    "    SIGMA = LAMBDA*np.diag(np.ones(dim))\n",
    "    BB = np.zeros(dim)\n",
    "    REGRET = 0\n",
    "    TOTALREGRET = []\n",
    "    for t in range(T):\n",
    "        weight = 1\n",
    "        noise= t_dist.rvs(df=3, size=actions)   * sigma\n",
    "        flag = 0\n",
    "        if cur_crr < corruption:\n",
    "       \n",
    "            flag = 1\n",
    "\n",
    "        decision = init_arms(dim, norm, actions)\n",
    "        rewardS = np.random.randn(actions)\n",
    "        reward = np.random.randn(actions)\n",
    "        optimal_reward = float(\"-inf\")\n",
    "        for arm in range(actions):\n",
    "            rewardS[arm] = noise[arm] + np.dot(decision[arm], bmu)\n",
    "            reward[arm] = rewardS[arm]\n",
    "            if np.dot(decision[arm], bmu) > optimal_reward:\n",
    "                optimal_reward = np.dot(decision[arm], bmu)\n",
    "        if flag == 1:\n",
    "            cur_crr += 1\n",
    "            for arm in range(actions):\n",
    "                reward[arm] = noise[arm] - np.dot(decision[arm], bmu)\n",
    "        hattheta = np.linalg.lstsq(SIGMA[:,:], BB[:], rcond=-1)\n",
    "        hattheta = hattheta[0]\n",
    "        hattheta.shape = (dim)\n",
    "        MAX_R = float(\"-inf\")\n",
    "        for a_t in range(actions):\n",
    "            action_t = decision[a_t]\n",
    "            UU=np.linalg.lstsq(SIGMA[:,:], action_t, rcond=-1)\n",
    "            UU= UU[0]\n",
    "            UU.shape = (dim)\n",
    "            r_action_t = np.dot(action_t, hattheta) + beta * np.sqrt(np.dot(UU,action_t))\n",
    "            if MAX_R < r_action_t: \n",
    "                MAX_R = r_action_t\n",
    "                final_a_t = a_t\n",
    "                action = action_t\n",
    "                weight = min(1,alpha/np.sqrt(np.dot(UU,action_t)))\n",
    "        REGRET += func(optimal_reward) - func(np.dot(action, bmu))\n",
    "        SIGMA[:,:]  = SIGMA[:,:] + weight*np.mat(action).T * np.mat(action)\n",
    "        BB[:] = BB[:] + weight*reward[final_a_t] * action\n",
    "        TOTALREGRET.append(REGRET)\n",
    "    path  = \"AdditiveC_lowerbound_\" + str(number) +\"_\"+ str(corruption)+\".txt\"\n",
    "    fr = open(path,'w')\n",
    "    for i in TOTALREGRET:\n",
    "        fr.write(str(i))\n",
    "        fr.write(\"\\n\")\n",
    "    fr.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jjjj in range(repeat):\n",
    "    \n",
    "    number =  jjjj-repeat_time*int(jjjj /repeat_time)\n",
    "    print(corruption, number)\n",
    "    cur_crr = 0\n",
    "    decision_t = init_arms(dim, norm, actions)\n",
    "    \n",
    "    LAMBDA = 1\n",
    "    beta=10\n",
    "    alpha=1\n",
    "    SIGMA = LAMBDA*np.diag(np.ones(dim))\n",
    "    BB = np.zeros(dim)\n",
    "    REGRET = 0\n",
    "    TOTALREGRET = []\n",
    "    for t in range(T):\n",
    "        noise= t_dist.rvs(df=3, size=actions)   * sigma\n",
    "        \n",
    "        flag = 0\n",
    "        if cur_crr < corruption:\n",
    "            decision = init_arms(dim, norm, actions)\n",
    "            if cur_crr < corruption:\n",
    "                flag = 1\n",
    "        else:\n",
    "            decision = decision_t\n",
    "       \n",
    "        decision = init_arms(dim, norm, actions)\n",
    "        rewardS = np.random.randn(actions)\n",
    "        reward = np.random.randn(actions)\n",
    "        optimal_reward = float(\"-inf\")\n",
    "        for arm in range(actions):\n",
    "            rewardS[arm] = noise[arm] + np.dot(decision[arm], bmu)\n",
    "            reward[arm] = rewardS[arm]\n",
    "            if np.dot(decision[arm], bmu) > optimal_reward:\n",
    "                optimal_reward = np.dot(decision[arm], bmu)\n",
    "        if flag == 1:\n",
    "            cur_crr += 1\n",
    "            for arm in range(actions):\n",
    "                reward[arm] = noise[arm] - np.dot(decision[arm], bmu)\n",
    "        hattheta = np.linalg.lstsq(SIGMA[:,:], BB[:], rcond=-1)\n",
    "        hattheta = hattheta[0]\n",
    "        hattheta.shape = (dim)\n",
    "        MAX_R = float(\"-inf\")\n",
    "        for a_t in range(actions):\n",
    "            action_t = decision[a_t]\n",
    "            r_action_t = np.dot(action_t, hattheta)\n",
    "            if MAX_R < r_action_t: \n",
    "                MAX_R = r_action_t\n",
    "                final_a_t = a_t\n",
    "                action = action_t\n",
    "        REGRET += func(optimal_reward) - func(np.dot(action, bmu))\n",
    "        SIGMA[:,:]  = SIGMA[:,:] + np.mat(action).T * np.mat(action)\n",
    "        BB[:] = BB[:] + reward[final_a_t] * action\n",
    "        TOTALREGRET.append(REGRET)\n",
    "    path  = \"Greedy_lowerbound_\" + str(number) +\"_\"+ str(corruption)+\".txt\"\n",
    "    fr = open(path,'w')\n",
    "    for i in TOTALREGRET:\n",
    "        fr.write(str(i))\n",
    "        fr.write(\"\\n\")\n",
    "    fr.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jjjj in range(repeat):\n",
    "    \n",
    "    number =  jjjj-repeat_time*int(jjjj /repeat_time)\n",
    "    print(corruption, number)\n",
    "    cur_crr = 0\n",
    "    decision_t = init_arms(dim, norm, actions)\n",
    "   \n",
    "    LAMBDA = 1\n",
    "    beta=1\n",
    "    alpha=1\n",
    "    SIGMA = LAMBDA*np.diag(np.ones(dim))\n",
    "    BB = np.zeros(dim)\n",
    "    REGRET = 0\n",
    "    TOTALREGRET = []\n",
    "    for t in range(T):\n",
    "        noise= t_dist.rvs(df=3, size=actions)   * sigma\n",
    "       \n",
    "        flag = 0\n",
    "        if cur_crr < corruption:\n",
    "            decision = init_arms(dim, norm, actions)\n",
    "            if cur_crr < corruption:\n",
    "                flag = 1\n",
    "        else:\n",
    "            decision = decision_t\n",
    "       \n",
    "        decision = init_arms(dim, norm, actions)\n",
    "        rewardS = np.random.randn(actions)\n",
    "        reward = np.random.randn(actions)\n",
    "        optimal_reward = float(\"-inf\")\n",
    "        for arm in range(actions):\n",
    "            rewardS[arm] = noise[arm] + np.dot(decision[arm], bmu)\n",
    "            reward[arm] = rewardS[arm]\n",
    "            if np.dot(decision[arm], bmu) > optimal_reward:\n",
    "                optimal_reward = np.dot(decision[arm], bmu)\n",
    "        if flag == 1:\n",
    "            cur_crr += 1\n",
    "            for arm in range(actions):\n",
    "                reward[arm] = noise[arm] - np.dot(decision[arm], bmu)\n",
    "        hattheta = np.linalg.lstsq(SIGMA[:,:], BB[:], rcond=-1)\n",
    "        hattheta = hattheta[0]\n",
    "        hattheta.shape = (dim)\n",
    "        MAX_R = float(\"-inf\")\n",
    "        for a_t in range(actions):\n",
    "            action_t = decision[a_t]\n",
    "            UU=np.linalg.lstsq(SIGMA[:,:], action_t, rcond=-1)\n",
    "            UU= UU[0]\n",
    "            UU.shape = (dim)\n",
    "            r_action_t = np.dot(action_t, hattheta) + beta * np.sqrt(np.dot(UU,action_t))\n",
    "            if MAX_R < r_action_t: \n",
    "                MAX_R = r_action_t\n",
    "                final_a_t = a_t\n",
    "                action = action_t\n",
    "        REGRET += func(optimal_reward) - func(np.dot(action, bmu))\n",
    "        SIGMA[:,:]  = SIGMA[:,:] + np.mat(action).T * np.mat(action)\n",
    "        BB[:] = BB[:] + reward[final_a_t] * action\n",
    "        TOTALREGRET.append(REGRET)\n",
    "    path  = \"OFUL_lowerbound_\" + str(number) +\"_\"+ str(corruption)+\".txt\"\n",
    "    fr = open(path,'w')\n",
    "    for i in TOTALREGRET:\n",
    "        fr.write(str(i))\n",
    "        fr.write(\"\\n\")\n",
    "    fr.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jjjj in range(repeat):\n",
    "    \n",
    "    number =  jjjj-repeat_time*int(jjjj /repeat_time)\n",
    "    print(corruption, number)\n",
    "    cur_crr = 1\n",
    "    decision_t = init_arms(dim, norm, actions)\n",
    "    \n",
    "    L=1\n",
    "    K=1\n",
    "    k=1\n",
    "    delta=0.001\n",
    "    lambda_= 0.1\n",
    "    #lambda_= dim/B**2\n",
    "    H = np.eye(dim) * lambda_\n",
    "    theta = np.zeros(dim)\n",
    "    beta = np.sqrt(lambda_)   \n",
    "    #m_0 = 1 / (6 * np.sqrt(3 * np.log(2 * T**2 / delta)))\n",
    "    m_0=1\n",
    "    m_1 = 1 / (42 * np.log(2 * T**2 / delta))\n",
    "    \n",
    "    sigma_min = 1 / np.sqrt(T)\n",
    "    print(sigma_min)\n",
    "    kappa = dim *np.log(1 + (T ) / (dim * lambda_ * sigma_min ** 2))\n",
    "    print(kappa)\n",
    "    #tau_0 = max(np.sqrt(2 * kappa), 2 * np.sqrt(dim)) / np.sqrt(np.log(2 * T**2 / delta))\n",
    "    tau_0=1\n",
    "    print(tau_0,m_0,m_1)\n",
    "    # Parameters for calculations.\n",
    "    alpha = max(np.sqrt(K) / (m_1 ** 0.25 * dim ** 0.25),0)\n",
    "    print(alpha)\n",
    "    sigma_ = [None] * (T + 1)\n",
    "    w_ = [None] * (T + 1)\n",
    "    tau_ = [None] * (T + 1)\n",
    "    y_ = [None] * (T + 1)  \n",
    "    var_=[None] * (T + 1)\n",
    "    phi_ = [None] * (T + 1)  \n",
    "    \n",
    "    \n",
    "   \n",
    "    REGRET = 0\n",
    "    TOTALREGRET = []\n",
    "    for t in range(1,T+1):\n",
    "        noise=np.random.randn(actions) *sigma\n",
    "        flag = 0\n",
    "        if cur_crr < corruption:\n",
    "            flag = 1\n",
    "\n",
    "        decision = init_arms(dim, norm, actions)\n",
    "        \n",
    "        rewardS = np.random.randn(actions)\n",
    "        reward = np.random.randn(actions)\n",
    "        optimal_reward = float(\"-inf\")\n",
    "        for arm in range(actions):\n",
    "            rewardS[arm] = noise[arm] + np.dot(decision[arm], bmu)\n",
    "            reward[arm] = rewardS[arm]\n",
    "            if np.dot(decision[arm], bmu) > optimal_reward:\n",
    "                optimal_reward = np.dot(decision[arm], bmu)\n",
    "        if flag == 1:\n",
    "            cur_crr += 1\n",
    "            for arm in range(actions):\n",
    "                reward[arm] = noise[arm] - np.dot(decision[arm], bmu)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # Find the best arm.\n",
    "        max_dot_product = float('-inf')  \n",
    "        best_i= None \n",
    "        for i in range(actions):\n",
    "        # Construct the confidence set.\n",
    "        \n",
    "            def objective(theta_):  \n",
    "                return -np.dot(decision[i], theta_)  \n",
    "            def constraint1(theta_):  \n",
    "                return B - np.linalg.norm(theta_)  \n",
    "\n",
    "            def constraint2(theta_):  \n",
    "                return beta - np.sqrt(np.dot(np.dot(theta_ - theta, H), theta_ - theta))  \n",
    "            \n",
    "            theta0 = np.zeros_like(theta)  \n",
    "\n",
    "            con1 = {'type': 'ineq', 'fun': constraint1}  \n",
    "            con2 = {'type': 'ineq', 'fun': constraint2}  \n",
    "            cons = [con1, con2]  \n",
    "\n",
    " \n",
    "            result = minimize(objective, theta0, constraints=cons)  \n",
    "\n",
    "            optimal_theta = result.x  \n",
    "            optimal_value = -result.fun  \n",
    "            if optimal_value > max_dot_product:  \n",
    "                max_dot_product = optimal_value \n",
    "                best_i = i\n",
    "       \n",
    "        \n",
    "        phi_[t]= decision[best_i]\n",
    "    \n",
    "        \n",
    "        \n",
    "        REGRET += func(optimal_reward) -func(np.dot(phi_[t], bmu))\n",
    "        \n",
    "        phi_t_H_inv_norm= np.sqrt(np.dot(np.dot(phi_[t], np.linalg.inv(H)),phi_[t]))\n",
    "        y_[t] = reward_function(phi_[t], bmu, flag)\n",
    "        \n",
    "\n",
    "        var_[t] = sigma\n",
    "        sigma_[t] = max(var_[t], sigma_min, phi_t_H_inv_norm / m_0, alpha * (phi_t_H_inv_norm ** 0.5))\n",
    "        #sigma_[t] = 1\n",
    "        w_[t]= phi_t_H_inv_norm/sigma_[t]\n",
    "        print(f'w_{t}={w_[t]}')\n",
    "        tau_[t]= tau_0 * np.sqrt(1 + w_[t] ** 2) / w_[t]\n",
    "        theta = minimize(fun=objective_function, x0=theta, args=(lambda_, B, k, t, phi_, y_, sigma_, tau_, func),jac=gradient_function,constraints=[{'type': 'ineq', 'fun': constraint12}]).x\n",
    "       \n",
    "\n",
    "        \n",
    "        \n",
    "        H += np.outer(phi_[t], phi_[t]) / (sigma_[t] ** 2)\n",
    "       \n",
    "        #beta = (64 / k) * (2 * np.sqrt(kappa * np.log(2 * t**2 / delta)) + np.sqrt(dim * np.log(2 * t**2 / delta)) )+ 5 * np.sqrt(dim) + (4 * np.sqrt(kappa) / k)\n",
    "        beta=1.\n",
    "        if t %10 == 0:\n",
    "            print(t)\n",
    "            print(np.linalg.norm(theta-bmu)) \n",
    "            print(theta)\n",
    "           \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        TOTALREGRET.append(REGRET)\n",
    "    \n",
    "    \n",
    "    path  = \"ADA_lowerbound_\" + str(number) +\"_\"+ str(corruption)+\".txt\"\n",
    "    fr = open(path,'w')\n",
    "    for i in TOTALREGRET:\n",
    "        fr.write(str(i))\n",
    "        fr.write(\"\\n\")\n",
    "    fr.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 50\n",
    "repeat = 5\n",
    "\n",
    "\n",
    "rewardrobust = np.zeros(T+1)\n",
    "rewardgreedy = np.zeros(T+1)\n",
    "rewardOFUL = np.zeros(T+1)\n",
    "rewardgada=np.zeros(T+1)\n",
    "rewardADA=np.zeros(T+1)\n",
    "rewardADAD=np.zeros(T+1)\n",
    "finalrewardrobust = np.zeros(11)\n",
    "finalrewardgreedy = np.zeros(11)\n",
    "finalrewardOFUL = np.zeros(11)\n",
    "for jjjj in range(repeat):\n",
    "    line1=0\n",
    "    path  = \"AdditiveC_lowerbound_\" + str(jjjj) +\"_\"+ str(number)+\".txt\"\n",
    "    fr = open(path,'r')\n",
    "    for line in fr:\n",
    "        rewardrobust[line1]=rewardrobust[line1]+float(line)\n",
    "        line1+=1\n",
    "    fr.close() \n",
    "for i in range(T):\n",
    "    rewardrobust[i] = rewardrobust[i]/repeat\n",
    "\n",
    "\n",
    "\n",
    "for jjjj in range(repeat):\n",
    "    line1=0\n",
    "    path  = \"Greedy_lowerbound_\" + str(jjjj) +\"_\"+ str(number)+\".txt\"\n",
    "    fr = open(path,'r')\n",
    "    for line in fr:\n",
    "        rewardgreedy[line1]=rewardgreedy[line1]+float(line)\n",
    "        line1+=1\n",
    "    fr.close() \n",
    "for i in range(T):\n",
    "    rewardgreedy[i] = rewardgreedy[i]/repeat\n",
    "\n",
    "for jjjj in range(repeat):\n",
    "    line1=0\n",
    "    path  = \"GADA_lowerbound_\" + str(jjjj) +\"_\"+ str(number)+\".txt\"\n",
    "    fr = open(path,'r')\n",
    "    for line in fr:\n",
    "        rewardgada[line1]=rewardgada[line1]+float(line)\n",
    "        line1+=1\n",
    "    fr.close() \n",
    "for i in range(T):\n",
    "    rewardgada[i] = rewardgada[i]/repeat\n",
    "\n",
    "for jjjj in range(repeat):\n",
    "    line1=0\n",
    "    path  = \"OFUL_lowerbound_\" + str(jjjj) +\"_\"+ str(number)+\".txt\"\n",
    "    fr = open(path,'r')\n",
    "    for line in fr:\n",
    "        rewardOFUL[line1]=rewardOFUL[line1]+float(line)\n",
    "        line1+=1\n",
    "    fr.close() \n",
    "for i in range(T):\n",
    "    rewardOFUL[i] = rewardOFUL[i]/repeat\n",
    "\n",
    "\n",
    "for jjjj in range(repeat):\n",
    "    line1=0\n",
    "    path  = \"ADA_lowerbound_\" + str(jjjj) +\"_\"+ str(number)+\".txt\"\n",
    "    fr = open(path,'r')\n",
    "    for line in fr:\n",
    "        rewardADA[line1]=rewardADA[line1]+float(line)\n",
    "        line1+=1\n",
    "    fr.close() \n",
    "for i in range(T):\n",
    "    rewardADA[i] = rewardADA[i]/repeat\n",
    "    \n",
    "\n",
    "\n",
    "x=list(range(1, T))\n",
    "\n",
    "x=list(range(1,T))\n",
    "plt.plot(x, rewardOFUL[0:T-1],color='black', label='OFUL',linestyle=\":\")\n",
    "    \n",
    "x=list(range(1, T))\n",
    "plt.plot(x, rewardgreedy[0:T-1],color='blue', label='Greedy',linestyle= \"--\")\n",
    "\n",
    "x=list(range(1, T))\n",
    "plt.plot(x, rewardrobust[0:T-1],color='green', label='CW-OFUL',linestyle= \"-.\")\n",
    "x=list(range(1, T))\n",
    "plt.plot(x, rewardgada[0:T-1],color='red', label='GAdaOFUL',linestyle= \"-\")\n",
    "x=list(range(1, T))\n",
    "plt.plot(x, rewardADA[0:T-1],color='orange', label='AdaOFUL(nonlinear)',linestyle= (0, (5, 1)))\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel('Number of Steps',size=10)\n",
    "plt.ylabel('Regret',size=10)\n",
    "\n",
    "plt.savefig('cor+nonlin1.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
